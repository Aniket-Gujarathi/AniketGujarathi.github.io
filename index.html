<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Aniket</title>

  <meta name="author" content="Aniket">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Aniket Sanjay Gujarathi</name>
              </p>
              <p>I am a graduate student at the University of Toronto pursuing an emphasis in robotics. Previously, I worked as an intern at Gatik.ai as a Software Engineer in the Perception Team.</a>
              I was also the Team Lead for the Radar Team at <a href="https://www.autodrive.utoronto.ca/">aUToronto</a> where I led a group of undergraduate and graduate students to develop algorithms for the perception stack of autonomous vehicles. 
              I also spent some time at Autonomous Space Robotics Lab (ASRL) under the guidance of <a href="http://asrl.utias.utoronto.ca/~tdb/">Prof. Tim Barfoot</a> where I worked on the project of deploying self-supervised lidar segmentation network on the Jackal Robot. I am interested in the general area of perception and deep-learning.
              </p>

              <p>
                Besides research, I enjoy singing, travelling and playing chess.

              </p>
<!--              <button onclick="research()">Research Interest</button>-->

              <p style="text-align:center">
                <a target="_blank" href="aniket.gujarathi@robotics.utias.utoronto.ca"> Email</a> &nbsp;/&nbsp;
                <a href="https://github.com/Aniket-Gujarathi">GitHub</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.co.in/citations?user=oz3PAHkAAAAJ&hl=en">Google Scholar</a>
                <a href="https://www.linkedin.com/in/aniket-gujarathi/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Aniket.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Aniket.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <!-- <h2>Background</h2> -->
        <p>
        <!-- with <a href="https://webdocs.cs.ualberta.ca/~dale/">Dale Schuurmans</a> and <a href="https://www.afaust.info/">Aleksandra Faust</a> in similar areas. -->
        <!-- I was fortunate to also work on consumer privacy rights and legislation with folks from <a href="https://www.law.georgetown.edu/privacy-technology-center">Georgetown's Center on Privacy and Technology.</a></p> -->
              </p>
              <p>
              </p>
        <h2>News</h2>

            <ul>
                    <li> <b> Dec 2022:</b> Completed the internship at Gatik.ai as a Software Engineer in the Perception Team.</li>
                    <li> <b> June 2022:</b> <a href="https://www.utoronto.ca/news/u-t-s-autoronto-team-wins-first-competition-autodrive-challenge-sequel">aUToronto</a> wins the first competition of the four-year AutoDrive Challenge 2 continuing the five year winning streak. </a></li>
                    <li> <b> May 2022:</b> Joining <a href="https://gatik.ai/">Gatik.ai</a> as a Software Engineering Intern with the Perception Team.</li>
                    <li> <b> Dec 2021:</b> Joining <a href="https://www.autodrive.utoronto.ca/">aUToronto</a> as the Radar Team Lead.</li>
                    <li> <b> Oct 2021:</b> Paper published in IROS 2021: <a href="https://ieeexplore.ieee.org/abstract/document/9636619">RoRD: Rotation Robust Descriptors and Orthographic Views for Local Feature Matching</a> </li>
                    <li> <b> Sept 2021:</b> Joining <a href="http://asrl.utias.utoronto.ca/">Autonomous Space Robotics Lab (ASRL)</a> as an MEng. student under the guidance of Prof. Tim Barfoot.</li>
                    <li> <b> Sept 2021:</b> Started MEng. at the University of Toronto Institute of Aerospace Studies with an emphasis in robotics.</li>
                  </ul>
            <!--<h2>Research 	&#129302;</h2>-->
        <h2>Projects</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                <!--
          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/todo.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="todo_link">
                <papertitle>TODO_paper_title</papertitle>
              </a>
              <br>
              <strong>Rose E. Wang</strong>,
              TODO,
              <a href="https://cocolab.stanford.edu/ndg">Noah Goodman</a>
              <br>
              <em>TODO conference venue</em>.
              <br>
              <p>TODO tldr</p>
            </td>
          </tr>
                -->
            <tr onmouseout="comp_attention_stop()" onmouseover="comp_attention_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/rord.png" alt="kts" style="border-style: none" width="200">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/9636619">
                  <papertitle>RoRD: Rotation-Robust Descriptors and Orthographic Views for Local Feature Matching</papertitle>
                </a>
                <br>
                Udit Singh Parihar, <strong>Aniket Gujarathi</strong>, Kinal Mehta, Satyajit Tourani, Sourav Garg, Michael Milford, and K. Madhava Krishna
                <br>
                <em>2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>.
                <br>
                [<a href="https://uditsinghparihar.github.io/RoRD/">Official Website</a>]
                <br>
                <p>
                  We present a novel framework that combines learning of invariant descriptors through data augmentation and orthographic viewpoint projection. We propose rotation-robust local descriptors, learnt through training data augmentation based on rotation homographies, and a correspondence ensemble technique that combines vanilla feature correspondences with those obtained through rotation-robust features. Using a range of benchmark datasets as well as contributing a new bespoke dataset for this research domain, we evaluate the effectiveness of the proposed approach on key tasks including pose estimation and visual place recognition.
    </p>
              </td>
            </tr>

          <tr onmouseout="comp_attention_stop()" onmouseover="comp_attention_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/delivery.jpg" alt="kts" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2103.09229.pdf">
                <papertitle>Design and Development of Autonomous Delivery Robot</papertitle>
              </a>
              <br>
              <strong>Aniket Gujarathi</strong>, Akshay Kulkarni, Unmesh Patil, Yogesh Phalak, Rajeshree Deotalu,
              Aman Jain, Navid Panchi, Ashwin Dhabale,
              and
              Shital S. Chiddarwar
              <br>
              <em>Capstone Project</em>.
              <br>
              [<a href="https://www.ivlabs.in/autonomous-driving-platform.html">Official Website</a>]
              <br>
              <p>
              We design and develop a low-cost autonomous delivery robot. The robot is equipped with multiple onboard sensors - Orbecc Astra Camera, YDLidar X4, IMU, and IR, for perception and state-estimation. The purpose of this project was to develop the autonomy stack for delivery robots.
</p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/stair.jpg" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8675676">
                  <papertitle>Deep Learning Based Stair Detection and Statistical Image Filtering for Autonomous Stair Climbing</papertitle>
              </a> 
              <br>
              Unmesh Patil, <strong>Aniket Gujarathi</strong>, Akshay Kulkarni, Aman Jain, Lokeshkumar Malke, Radhika Tekade, Kartik Paigwar, and Pradyumn Chaturvedi
              <br>
              <em>IEEE International Conference on Robotic Computing (IRC)</em>
              <br>
              <a href="https://www.worldscientific.com/doi/abs/10.1142/S1793351X1940021X"> <papertitle>Deep Learning-Based Stair Segmentation and Behavioral Cloning for Autonomous Stair Climbing</papertitle>
              </a> 
              <br>
              Navid Panchi, Khush Agrawal, Unmesh Patil, <strong>Aniket Gujarathi</strong>, Aman Jain, Harsha Namdeo, and Shital S Chiddarwar
              <br>
              <em>International Journal of Semantic Computing</em>
                <br>
              [<a href="https://www.ivlabs.in/autonomous-multi-storey-surveillance-robot.html">Official Website</a>]
              <br>
              <p>In this work, we design a multi-storey surveillance robot that works on a novel distributed push-pull mechanism to climb stairs. We also add a camera to assist in detecting the stairs and estimating the relative pose to ensure accurate navigation. We further extend our idea to use stair segmentation and behavioral cloning for stair climbing. </p>
            </td>
          </tr>
  
          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/autoexmap.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Autonomous Exploration and Simultaneous 3D Map Generation</papertitle>
              <br>
                <strong>Aniket Gujarathi</strong> and Unmesh Patil
                <br>
              <em>IvLabs Project</em>
                <br>
              [
              <a href="https://www.ivlabs.in/turtlebot-2.html">Official Website</a>]
                <br>
                <p>
                    In this work, we use a TurtleBot to explore an unseen environment and generate a 3D map using RTABMap in ROS. This aim of this project is to learn ROS and deploy ROS packages on a real robot. 
                </p>
            </td>
          </tr>

        </tbody></table>

      </td>
    </tr>
  </table>
  Template from <a href="https://jonbarron.info/">this website</a>.
</body>

</html>
